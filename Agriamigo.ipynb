{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fa0b7b2-defc-48c9-a9ab-7f147bd2e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found dataset: PlantVillage at /Volumes/RAY/CropMoiniteringdata/Healthy crop /PlantVillage\n",
      "✅ Found dataset: CCMT at /Volumes/RAY/CropMoiniteringdata/CCMT_FInal Dataset\n",
      "✅ Found dataset: PlantDoc at /Volumes/RAY/CropMoiniteringdata/PlantDoc-Dataset/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the selected dataset paths\n",
    "datasets = {\n",
    "    \"PlantVillage\": r\"/Volumes/RAY/CropMoiniteringdata/Healthy crop /PlantVillage\",\n",
    "    \"CCMT\": r\"/Volumes/RAY/CropMoiniteringdata/CCMT_FInal Dataset\",\n",
    "    \"PlantDoc\": r\"/Volumes/RAY/CropMoiniteringdata/PlantDoc-Dataset/train\"\n",
    "}\n",
    "\n",
    "# Check if each dataset path exists\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"✅ Found dataset: {dataset_name} at {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: Dataset not found: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e4569be-51a5-4912-8bae-67e925881837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 PlantVillage: 11874 images found\n",
      "📂 CCMT: 18697 images found\n",
      "📂 PlantDoc: 2316 images found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to count images in each dataset folder\n",
    "def count_images(folder_path):\n",
    "    total_images = 0\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        total_images += len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))])\n",
    "    return total_images\n",
    "\n",
    "# Check image counts for each dataset\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        num_images = count_images(dataset_path)\n",
    "        print(f\"📂 {dataset_name}: {num_images} images found\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: Dataset folder not found - {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3927986f-a410-4586-ae5b-7f95bd5fd642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created category folders inside: CropMonitoringData_Sorted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory for sorted images\n",
    "target_base_dir = \"CropMonitoringData_Sorted\"\n",
    "\n",
    "# Define categories\n",
    "categories = [\"Healthy\", \"Diseased\", \"Pest\", \"Nutrient_Deficiency\", \"Moisture_Stress\"]\n",
    "\n",
    "# Ensure each category folder exists\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(target_base_dir, category), exist_ok=True)\n",
    "\n",
    "print(f\"✅ Created category folders inside: {target_base_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b26eb020-e5f7-442a-adbf-8f30a71dad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moving images from: PlantVillage\n",
      "✅ Moving images from: CCMT\n",
      "✅ Moving images from: PlantDoc\n",
      "🎉 All images have been successfully organized into categories!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Mapping dataset folder names to our categories\n",
    "folder_mapping = {\n",
    "    # Healthy Crops\n",
    "    \"Tomato_healthy\": \"Healthy\",\n",
    "    \"Pepper__bell___healthy\": \"Healthy\",\n",
    "\n",
    "    # Diseased Crops\n",
    "    \"Tomato_Bacterial_spot\": \"Diseased\",\n",
    "    \"Tomato_Early_blight\": \"Diseased\",\n",
    "    \"Tomato_Late_blight\": \"Diseased\",\n",
    "    \"Tomato_Septoria_leaf_spot\": \"Diseased\",\n",
    "    \"Tomato_Target_Spot\": \"Diseased\",\n",
    "    \"Tomato_Tomato_YellowLeaf_Curl_Virus\": \"Diseased\",\n",
    "    \"Tomato_Tomato_mosaic_virus\": \"Diseased\",\n",
    "    \"Potato_Early_blight\": \"Diseased\",\n",
    "    \"Potato_Late_blight\": \"Diseased\",\n",
    "    \"Pepper__bell___Bacterial_spot\": \"Diseased\",\n",
    "\n",
    "    # Pest Infestation\n",
    "    \"Tomato_Spider_mites_Two_spotted_spider_mite\": \"Pest\",\n",
    "\n",
    "    # Nutrient Deficiency (Need Verification)\n",
    "    \"bacterial_leaf_blight\": \"Nutrient_Deficiency\",\n",
    "    \"bacterial_leaf_streak\": \"Nutrient_Deficiency\",\n",
    "    \"bacterial_panicle_blight\": \"Nutrient_Deficiency\",\n",
    "\n",
    "    # Moisture Stress\n",
    "    \"dead_heart\": \"Moisture_Stress\",\n",
    "    \"downy_mildew\": \"Moisture_Stress\",\n",
    "    \"blast\": \"Moisture_Stress\",\n",
    "    \"brown_spot\": \"Moisture_Stress\",\n",
    "    \"hispa\": \"Moisture_Stress\",\n",
    "    \"tungro\": \"Moisture_Stress\"\n",
    "}\n",
    "\n",
    "# Function to move images from dataset folders to the correct category\n",
    "def move_images(source_folder, mapping):\n",
    "    for root, dirs, files in os.walk(source_folder):  # Recursively find all files\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):  # Only move valid images\n",
    "                folder_name = os.path.basename(root)  # Get last folder name\n",
    "                category = mapping.get(folder_name, None)\n",
    "\n",
    "                if category:\n",
    "                    src_file_path = os.path.join(root, file)\n",
    "                    dest_folder_path = os.path.join(target_base_dir, category)\n",
    "\n",
    "                    os.makedirs(dest_folder_path, exist_ok=True)  # Ensure category folder exists\n",
    "                    shutil.move(src_file_path, os.path.join(dest_folder_path, file))\n",
    "\n",
    "# Move images from the selected datasets\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"✅ Moving images from: {dataset_name}\")\n",
    "        move_images(dataset_path, folder_mapping)\n",
    "    else:\n",
    "        print(f\"❌ Dataset not found: {dataset_name}\")\n",
    "\n",
    "print(\"🎉 All images have been successfully organized into categories!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f83c9836-4d4a-4078-9cfe-8cee9d8f47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Diseased: 9804 images\n",
      "📂 Pest: 1676 images\n",
      "📂 Healthy: 3691 images\n",
      "📂 Moisture_Stress: 0 images\n",
      "📂 Nutrient_Deficiency: 0 images\n"
     ]
    }
   ],
   "source": [
    "for category in os.listdir(target_base_dir):\n",
    "    category_path = os.path.join(target_base_dir, category)\n",
    "    \n",
    "    if os.path.isdir(category_path):  # Ensure it's a valid directory\n",
    "        num_images = len(os.listdir(category_path))\n",
    "        print(f\"📂 {category}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cb91e24-d929-4f91-b4e1-fc4160cc42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Checking dataset: PlantVillage\n",
      "\n",
      "📂 Checking dataset: CCMT\n",
      "\n",
      "📂 Checking dataset: PlantDoc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check for Moisture_Stress and Nutrient_Deficiency images in the original datasets\n",
    "missing_categories = [\"Moisture_Stress\", \"Nutrient_Deficiency\"]\n",
    "\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    print(f\"\\n📂 Checking dataset: {dataset_name}\")\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        for folder in os.listdir(dataset_path):\n",
    "            folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "            if os.path.isdir(folder_path) and folder in missing_categories:  \n",
    "                num_images = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "                print(f\"   - {folder}: {num_images} images\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: Dataset folder not found - {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc958c42-5f98-4349-8aa7-92d290a8d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Removed empty folder: CropMonitoringData_Sorted/Moisture_Stress\n",
      "🗑️ Removed empty folder: CropMonitoringData_Sorted/Nutrient_Deficiency\n",
      "✅ Only Healthy, Diseased, and Pest categories remain.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the categories to remove\n",
    "empty_categories = [\"Moisture_Stress\", \"Nutrient_Deficiency\"]\n",
    "\n",
    "# Remove empty folders\n",
    "for category in empty_categories:\n",
    "    category_path = os.path.join(target_base_dir, category)\n",
    "    if os.path.exists(category_path):\n",
    "        shutil.rmtree(category_path)\n",
    "        print(f\"🗑️ Removed empty folder: {category_path}\")\n",
    "\n",
    "print(\"✅ Only Healthy, Diseased, and Pest categories remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae7e1d59-6f06-4e40-aa5f-9fe905fef281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Automatically use MPS if available, otherwise fallback to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b75172a0-bd95-43f3-a467-711839a0f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "✅ Model moved to: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ✅ Set device to MPS (Mac GPU) if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ✅ Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# ✅ Modify the last layer to match your number of classes\n",
    "num_classes = 2  # Change this based on your dataset (e.g., Healthy & Diseased)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# ✅ Move the model to MPS (Mac GPU)\n",
    "model = model.to(device)\n",
    "print(\"✅ Model moved to:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb10ac9a-047b-4920-b2c9-2ec867eb6b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CropMonitoringData_Sorted exists.\n",
      "📂 Contents: ['Diseased', 'Pest', 'Healthy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"CropMonitoringData_Sorted\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(f\"✅ {base_dir} exists.\")\n",
    "    print(\"📂 Contents:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"❌ ERROR: {base_dir} not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51219a17-226f-48d5-95fe-07f651e0e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Images successfully moved into 'train/' and 'val/' folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"CropMonitoringData_Sorted\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "\n",
    "# Create train and val directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Categories to process (Now including 'Pest')\n",
    "categories = [\"Diseased\", \"Healthy\", \"Pest\"]\n",
    "\n",
    "for category in categories:\n",
    "    src_folder = os.path.join(base_dir, category)  # Original dataset folder\n",
    "    train_dest = os.path.join(train_dir, category)  # Train folder\n",
    "    val_dest = os.path.join(val_dir, category)  # Validation folder\n",
    "\n",
    "    os.makedirs(train_dest, exist_ok=True)\n",
    "    os.makedirs(val_dest, exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(src_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    random.shuffle(images)  # Shuffle images to ensure randomness\n",
    "\n",
    "    split_idx = int(len(images) * 0.8)  # 80% Train, 20% Validation\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        src_path = os.path.join(src_folder, img)\n",
    "        dest_path = os.path.join(train_dest if i < split_idx else val_dest, img)\n",
    "        shutil.move(src_path, dest_path)\n",
    "\n",
    "print(\"✅ Images successfully moved into 'train/' and 'val/' folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e078036-baf2-43bd-a518-882942c4c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 ['Diseased', 'Pest', 'Healthy', 'train', 'val']\n",
      "📂 Train: ['Diseased', 'Pest', 'Healthy']\n",
      "📂 Val: ['Diseased', 'Pest', 'Healthy']\n"
     ]
    }
   ],
   "source": [
    "print(\"📂\", os.listdir(\"CropMonitoringData_Sorted\"))\n",
    "print(\"📂 Train:\", os.listdir(\"CropMonitoringData_Sorted/train\"))\n",
    "print(\"📂 Val:\", os.listdir(\"CropMonitoringData_Sorted/val\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09df95cc-5f51-4a11-a13a-374eb1c686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Images: 12135\n",
      "✅ Validation Images: 3036\n",
      "✅ Classes: ['Diseased', 'Healthy', 'Pest']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ✅ Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=\"CropMonitoringData_Sorted/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"CropMonitoringData_Sorted/val\", transform=transform)\n",
    "\n",
    "print(f\"✅ Training Images: {len(train_dataset)}\")\n",
    "print(f\"✅ Validation Images: {len(val_dataset)}\")\n",
    "print(f\"✅ Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84a37e9b-94d3-452f-8145-074466d9cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train DataLoader: 380 batches\n",
      "✅ Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# ✅ Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✅ Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"✅ Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "609e12ce-a5a2-485a-90ca-ad9a46b2b4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model moved to: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ✅ Load Pretrained ResNet18\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Use MPS on Mac\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the final layer for 3 classes (Diseased, Healthy, Pest)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 3)  # 3 output classes\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"✅ Model moved to: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94ffc8c6-1ab2-41e0-a72d-4f1dccbf3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# ✅ Define Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"✅ Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b20e1d56-5a05-4150-940d-80911c300821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train DataLoader: 380 batches\n",
      "✅ Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# ✅ Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ✅ Custom Dataset Class with Error Handling\n",
    "class CustomImageDataset(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            path, target = self.samples[index]\n",
    "            image = Image.open(path).convert(\"RGB\")  # Convert to RGB to avoid mode issues\n",
    "            image = self.transform(image)\n",
    "            return image, target\n",
    "        except (OSError, Image.DecompressionBombError) as e:\n",
    "            print(f\"⚠️ Skipping corrupt image: {path}\")\n",
    "            return self.__getitem__((index + 1) % len(self.samples))  # Skip and move to next\n",
    "\n",
    "# ✅ Load datasets\n",
    "train_dataset = CustomImageDataset(root=\"CropMonitoringData_Sorted/train\", transform=transform)\n",
    "val_dataset = CustomImageDataset(root=\"CropMonitoringData_Sorted/val\", transform=transform)\n",
    "\n",
    "# ✅ Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"✅ Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"✅ Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f38ba86b-2766-4672-a800-af05f4298004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train DataLoader: 380 batches\n",
      "✅ Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "# ✅ Create DataLoaders (Set num_workers=0 for macOS compatibility)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"✅ Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"✅ Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1435c075-079c-4de2-822a-81871762f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|   | 8/380 [00:02<01:37,  3.80it/s, accuracy=96.1, loss=0.0262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   5%|  | 20/380 [00:05<01:28,  4.05it/s, accuracy=96.7, loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  23%|▋  | 88/380 [00:22<01:11,  4.08it/s, accuracy=96.9, loss=0.091]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  45%|▍| 172/380 [00:43<00:50,  4.10it/s, accuracy=97.7, loss=0.0258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█| 380/380 [01:37<00:00,  3.90it/s, accuracy=97.7, loss=0.00101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 complete. Loss: 0.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  19%|▍ | 74/380 [00:18<01:15,  4.05it/s, accuracy=97.9, loss=0.0551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  58%|▌| 220/380 [00:54<00:39,  4.09it/s, accuracy=98.7, loss=0.0472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  77%|▊| 293/380 [01:12<00:21,  4.12it/s, accuracy=98.7, loss=0.00168"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  97%|▉| 367/380 [01:30<00:03,  4.12it/s, accuracy=98.6, loss=0.00542"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█| 380/380 [01:33<00:00,  4.07it/s, accuracy=98.5, loss=0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 complete. Loss: 0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  18%|▏| 69/380 [00:16<01:14,  4.16it/s, accuracy=98.4, loss=0.00915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  30%|▎| 114/380 [00:27<01:06,  4.01it/s, accuracy=98.7, loss=0.0146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  48%|▍| 183/380 [00:44<00:47,  4.14it/s, accuracy=99.1, loss=0.00136"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  95%|█▉| 362/380 [01:28<00:04,  4.11it/s, accuracy=98.8, loss=0.027]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█| 380/380 [01:32<00:00,  4.10it/s, accuracy=98.8, loss=0.00726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 complete. Loss: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   6%|  | 23/380 [00:05<01:27,  4.08it/s, accuracy=99.2, loss=0.0268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  27%|▎| 103/380 [00:25<01:08,  4.02it/s, accuracy=99.2, loss=0.0086]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  44%|▍| 168/380 [00:41<00:51,  4.14it/s, accuracy=99.1, loss=0.00565"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  68%|██ | 258/380 [01:03<00:29,  4.14it/s, accuracy=99, loss=0.0287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█| 380/380 [01:32<00:00,  4.09it/s, accuracy=98.9, loss=0.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 complete. Loss: 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  32%|▎| 122/380 [00:29<01:02,  4.11it/s, accuracy=99.1, loss=0.0197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  40%|▍| 152/380 [00:37<00:57,  3.94it/s, accuracy=99.2, loss=0.00053"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  73%|█▍| 277/380 [01:07<00:25,  4.11it/s, accuracy=99.2, loss=0.054]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  79%|▊| 302/380 [01:13<00:19,  4.10it/s, accuracy=99.1, loss=0.0154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█| 380/380 [01:32<00:00,  4.09it/s, accuracy=99.2, loss=0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 complete. Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  16%|▏| 62/380 [00:15<01:18,  4.05it/s, accuracy=99.6, loss=0.00129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  49%|▍| 188/380 [00:46<00:46,  4.11it/s, accuracy=99.7, loss=0.00294"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  69%|▋| 264/380 [01:04<00:28,  4.11it/s, accuracy=99.6, loss=0.0215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  84%|▊| 319/380 [01:18<00:14,  4.10it/s, accuracy=99.6, loss=0.00035"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█| 380/380 [01:33<00:00,  4.08it/s, accuracy=99.6, loss=1.09e-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 complete. Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  15%|▎ | 56/380 [00:13<01:19,  4.09it/s, accuracy=99.3, loss=0.0355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  40%|▍| 153/380 [00:37<00:55,  4.12it/s, accuracy=99.3, loss=0.00411"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  73%|▋| 278/380 [01:08<00:24,  4.10it/s, accuracy=99.3, loss=0.00923"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  84%|▊| 321/380 [01:18<00:14,  4.10it/s, accuracy=99.3, loss=0.0138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█| 380/380 [01:33<00:00,  4.08it/s, accuracy=99.3, loss=0.00715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 complete. Loss: 0.0228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  30%|▌ | 113/380 [00:27<01:04,  4.14it/s, accuracy=99, loss=0.00335]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  56%|▌| 211/380 [00:51<00:41,  4.10it/s, accuracy=99.3, loss=0.00058"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  59%|▌| 226/380 [00:55<00:37,  4.08it/s, accuracy=99.3, loss=0.00374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  66%|▋| 251/380 [01:01<00:31,  4.10it/s, accuracy=99.3, loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█| 380/380 [01:32<00:00,  4.10it/s, accuracy=99.4, loss=0.00071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 complete. Loss: 0.0207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   1%|    | 4/380 [00:00<01:31,  4.11it/s, accuracy=100, loss=0.0017]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  13%|▏| 49/380 [00:11<01:20,  4.14it/s, accuracy=99.5, loss=0.000291"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  96%|▉| 364/380 [01:29<00:03,  4.11it/s, accuracy=99.3, loss=0.00046"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█| 380/380 [01:33<00:00,  4.08it/s, accuracy=99.3, loss=0.00141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 complete. Loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  11%| | 43/380 [00:10<01:20,  4.16it/s, accuracy=99.9, loss=0.00022"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  23%|▏| 89/380 [00:21<01:16,  3.79it/s, accuracy=99.8, loss=0.00083"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  29%|▎| 109/380 [00:26<01:05,  4.14it/s, accuracy=99.7, loss=0.0003"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  56%|▌| 211/380 [00:51<00:41,  4.10it/s, accuracy=99.6, loss=0.0446"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy442_.jpg\n",
      "⚠️ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy443_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█| 380/380 [01:32<00:00,  4.10it/s, accuracy=99.5, loss=0.0030"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 complete. Loss: 0.0156\n",
      "🎉 Training finished successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10  # You can adjust this\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# ✅ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move to MPS\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    print(f\"✅ Epoch {epoch+1} complete. Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"🎉 Training finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5377963-dd40-4e7a-ab2b-59ec34324db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Accuracy on Validation Set: 98.58%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"✅ Model Accuracy on Validation Set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e99ada96-4839-4580-9792-870ad6a696fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"crop_disease_classifier.pth\")\n",
    "print(\"✅ Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02d9b981-b12e-4acb-9845-143f67032ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Predicted Class: Diseased\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_path = \"/Volumes/RAY/LeafSamples/leaf2.jpg\"  # Update with actual image path\n",
    "image = Image.open(img_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "print(f\"🟢 Predicted Class: {train_dataset.classes[predicted.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffa266-ed3a-4c29-990e-320db9d0c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
